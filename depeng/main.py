import os  # å¯¼å…¥ os æ¨¡å—ï¼Œç”¨äºæ“ä½œç³»ç»Ÿç›¸å…³åŠŸèƒ½ï¼ˆå¦‚æ£€æŸ¥æ–‡ä»¶å­˜åœ¨ã€è·å–ç¯å¢ƒå˜é‡ï¼‰
import torch  # å¯¼å…¥ PyTorch åº“ï¼Œç”¨äºè®¾å¤‡æ£€æµ‹ï¼ˆCPU/CUDAï¼‰å’Œåº•å±‚æ¨¡å‹æ“ä½œ
from langchain_community.document_loaders import PyPDFLoader, TextLoader  # å¯¼å…¥ LangChain ç¤¾åŒºåº“çš„æ–‡æ¡£åŠ è½½å™¨
from langchain.text_splitter import RecursiveCharacterTextSplitter  # å¯¼å…¥é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨
from langchain_huggingface import HuggingFaceEmbeddings  # å¯¼å…¥ Hugging Face åµŒå…¥æ¨¡å‹ç±»ï¼ˆç”¨äºæœ¬åœ°æ¨¡å‹ï¼‰
from langchain_community.vectorstores import Chroma  # å¯¼å…¥ Chroma å‘é‡æ•°æ®åº“ç±»
from langchain_community.chat_models import ChatOpenAI  # å¯¼å…¥ ChatOpenAI ç±»ï¼ˆå…¼å®¹OpenAIåè®®çš„æ¨¡å‹æ¥å£ï¼‰
from langchain.prompts import ChatPromptTemplate  # å¯¼å…¥èŠå¤©æç¤ºæ¨¡æ¿ç±»
from langchain.schema.output_parser import StrOutputParser  # å¯¼å…¥å­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨
from langchain.schema.runnable import RunnablePassthrough  # å¯¼å…¥ RunnablePassthroughï¼Œç”¨äº LCEL ç®¡é“ä¸­ä¼ é€’è¾“å…¥


if os.path.exists("./data/ä¸‰å…¨é£Ÿå“_2023å¹´å¹´åº¦æŠ¥å‘Š_text.txt"):
    print("Yes")  # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ‰“å° "Yes"
device = "cuda" if torch.cuda.is_available() else "cpu"  # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ NVIDIA GPUï¼Œå†³å®šä½¿ç”¨ "cuda" æˆ– "cpu" è®¾å¤‡

# 1. åŠ è½½æ•°æ®
loader = TextLoader("./data/ä¸‰å…¨é£Ÿå“_2023å¹´å¹´åº¦æŠ¥å‘Š_text.txt", encoding='utf-8')  # å®ä¾‹åŒ– TextLoaderï¼ŒæŒ‡å®šè¦åŠ è½½çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„å’Œç¼–ç 
# å¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼šloader = TextLoader("my_document.txt", encoding="utf-8")  # ç¤ºä¾‹æ³¨é‡Šï¼šå¦‚ä½•åŠ è½½å¦ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶
documents = loader.load()  # æ‰§è¡ŒåŠ è½½æ“ä½œï¼Œè·å–æ–‡æ¡£ä¿¡æ¯


# 2. åˆ†å‰²æ–‡æ¡£ä¸­çš„æ•°æ®ä¸ºä¸€ä¸ªä¸ªæ–‡æœ¬å—
text_splitter = RecursiveCharacterTextSplitter(  # å®ä¾‹åŒ–é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨
    chunk_size=1000,  # è®¾ç½®æ¯ä¸ªæ–‡æ¡£å—çš„æœ€å¤§é•¿åº¦ï¼ˆæŒ‰å­—ç¬¦æˆ– Token è®¡ï¼Œå–å†³äºæ¨¡å‹ï¼‰
    chunk_overlap=200,  # è®¾ç½®ç›¸é‚»æ–‡æ¡£å—ä¹‹é—´çš„é‡å é•¿åº¦ï¼Œæœ‰åŠ©äºä¿ç•™ä¸Šä¸‹æ–‡
    length_function=len,  # è®¾ç½®è®¡ç®—é•¿åº¦çš„å‡½æ•°ï¼ˆè¿™é‡Œæ˜¯æ ‡å‡† Python len()ï¼‰
    add_start_index=True,  # æ˜¯å¦åœ¨å…ƒæ•°æ®ä¸­æ·»åŠ å—åœ¨åŸæ–‡æ¡£ä¸­çš„èµ·å§‹ç´¢å¼•
)
splits = text_splitter.split_documents(documents)  # æ‰§è¡Œåˆ†å‰²æ“ä½œï¼Œç”Ÿæˆå°å— Document åˆ—è¡¨

# 3. å»ºç«‹ç´¢å¼• (åˆ›å»ºåµŒå…¥å’Œå‘é‡å­˜å‚¨)
model_name = "moka-ai/m3e-base"  # æŒ‡å®šä½¿ç”¨çš„ Hugging Face åµŒå…¥æ¨¡å‹åç§°ï¼ˆä¸­æ–‡å¸¸ç”¨æ¨¡å‹ï¼‰

# åˆ›å»ºåµŒå…¥æ¨¡å‹
embedding = HuggingFaceEmbeddings(  # å®ä¾‹åŒ– HuggingFaceEmbeddings
    model_name=model_name,  # ä¼ å…¥æ¨¡å‹åç§°
    # å¯ç”¨ CUDA ä»¥åŠ é€Ÿè®¡ç®— (å¦‚æœæ‚¨çš„æœºå™¨æœ‰ GPU)
    model_kwargs={'device': device}  # å°†å‰é¢æ£€æµ‹åˆ°çš„è®¾å¤‡ï¼ˆ'cuda'æˆ–'cpu'ï¼‰ä¼ é€’ç»™æ¨¡å‹
)

vectorstore = Chroma.from_documents(  # ä½¿ç”¨åˆ†å‰²åçš„æ–‡æ¡£å—å’ŒåµŒå…¥æ¨¡å‹åˆ›å»º Chroma å‘é‡å­˜å‚¨
    documents=splits,  # ä¼ å…¥åˆ†å‰²åçš„æ–‡æ¡£å—
    embedding=embedding,  # ä¼ å…¥åµŒå…¥æ¨¡å‹
    persist_directory="./chroma_db"  # è®¾ç½®å‘é‡æ•°æ®åº“çš„æŒä¹…åŒ–å­˜å‚¨ç›®å½•
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # å°†å‘é‡å­˜å‚¨è½¬æ¢ä¸ºæ£€ç´¢å™¨ï¼Œå¹¶è®¾ç½®æ£€ç´¢å‚æ•° k=3ï¼ˆæ¯æ¬¡æ£€ç´¢è¿”å› 3 ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—ï¼‰

# åˆ›å»ºç³»ç»Ÿæç¤ºè¯
template = """
æ‚¨æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚  # è®¾å®š AI åŠ©æ‰‹çš„è§’è‰²
è¯·ä»…æ ¹æ®æä¾›çš„ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚  # é™åˆ¶æ¨¡å‹åªèƒ½ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè¯·è¯´æ‚¨ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚ # é¿å…å¹»è§‰ï¼ˆHallucinationï¼‰
è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚

ä¸Šä¸‹æ–‡:  # ä¸Šä¸‹æ–‡å ä½ç¬¦
{context}

é—®é¢˜:  # é—®é¢˜å ä½ç¬¦
{question}
"""
prompt = ChatPromptTemplate.from_template(template)  # ä½¿ç”¨ä¸Šè¿°æ¨¡æ¿åˆ›å»º ChatPromptTemplate å®ä¾‹ï¼Œç”¨æ¥å‚¨å­˜æç¤ºè¯ï¼Œæ–¹ä¾¿åç»­æ“ä½œ

# è®¿é—®å¹¶è¿æ¥å¤§è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(  # å®ä¾‹åŒ– ChatOpenAI ç±»ï¼Œç”¨äºè°ƒç”¨å…¼å®¹ OpenAI åè®®çš„æ¨¡å‹æœåŠ¡
    model='æ¨¡å‹åç§°ï¼Œéœ€å¡«å…¥',  # æŒ‡å®šè¦è°ƒç”¨çš„æ¨¡å‹åç§°
    base_url='æ¨¡å‹è®¿é—®åœ°å€ï¼Œéœ€å¡«å…¥',  # æŒ‡å®šè‡ªå®šä¹‰çš„ API åœ°å€ï¼ˆBase URLï¼‰
    api_key='æ¨¡å‹çš„å¯†é’¥ï¼Œéœ€å¡«å…¥',  # ä¼ å…¥è‡ªå®šä¹‰çš„ API Key
    temperature=0.2,  # è®¾ç½®æ¨¡å‹çš„éšæœºæ€§/åˆ›é€ æ€§ï¼Œè¾ƒä½çš„å€¼ï¼ˆ0.2ï¼‰ä½¿å¾—å›ç­”æ›´ç¨³å®šã€æ›´å¿ å®äºä¸Šä¸‹æ–‡
)

def format_docs(docs):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ ¼å¼åŒ–æ£€ç´¢å™¨è¿”å›çš„æ–‡æ¡£åˆ—è¡¨
    return "\n\n".join(doc.page_content for doc in docs)  # æå–æ¯ä¸ªæ–‡æ¡£å—çš„çº¯æ–‡æœ¬å†…å®¹ï¼ŒæŠŠragæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æœ¬å—è¿æ¥åˆ°ä¸€èµ·

# æ„å»º RAG Chain
rag_chain = (  # ä½¿ç”¨ LangChain Expression Language (LCEL) æ„å»º RAG ç®¡é“
    {"context": retriever | format_docs, "question": RunnablePassthrough()}  # å®šä¹‰è¾“å…¥æ˜ å°„ï¼šcontext ç”±æ£€ç´¢å™¨å’Œæ ¼å¼åŒ–å‡½æ•°ç”Ÿæˆï¼›question ç”±ç”¨æˆ·è¾“å…¥ç›´æ¥ç©¿é€ä¼ é€’
    | prompt  # å°†æ ¼å¼åŒ–çš„ context å’Œ question å¡«å…¥ prompt æ¨¡æ¿
    | llm  # å°†å®Œæ•´çš„æç¤ºè¯å‘é€ç»™ LLM è¿›è¡Œç”Ÿæˆ
    | StrOutputParser()  # è§£æ LLM çš„è¾“å‡ºï¼Œåªè¿”å›çº¯æ–‡æœ¬å­—ç¬¦ä¸²
)

# è¿è¡Œ RAG Chain
question = "ä¸‰å…¨é£Ÿå“å¹´åº¦æŠ¥å‘Šæåˆ°äº†å“ªäº›å…³äºæœªæ¥å‘å±•æˆ˜ç•¥çš„ä¿¡æ¯ï¼Ÿ"  # å®šä¹‰ç”¨æˆ·è¦æå‡ºçš„é—®é¢˜
response = rag_chain.invoke(question)  # æ‰§è¡Œæ•´ä¸ª RAG ç®¡é“ï¼Œè·å–æœ€ç»ˆå›ç­”ï¼ˆæ­¤è¡Œä»£ç æ˜¯è¿è¡Œä»£ç ï¼‰
print("\nğŸ“ AI åŠ©æ‰‹çš„å›ç­”:")  # æ‰“å°æç¤ºä¿¡æ¯
print(response)  # æ‰“å° LLM ç”Ÿæˆçš„æœ€ç»ˆå›ç­”